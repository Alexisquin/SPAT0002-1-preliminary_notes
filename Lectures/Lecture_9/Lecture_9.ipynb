{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian inference: part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Frequentist vs Bayesian Approaches\n",
    "\n",
    "Both the model fitting and model selection problems can be approached from either a *frequentist* or a *Bayesian* standpoint.\n",
    "Fundamentally, the difference between these lies in the **definition of probability** that they use:\n",
    "\n",
    "- **A frequentist probability is a measure *long-run frequency* of (real or imagined) repeated trials.** Among other things, this generally means that observed data can be described probabilistically (you can repeat the experiment and get a different realization) while model parameters are fixed, and cannot be described probabilistically (the universe remains the same no matter how many times you observe it). \n",
    "\n",
    "- **A Bayesian probability is a *quantification of belief*.** Among other things, this generally means that observed data are treated as fixed (you know exactly what values you measured) while model parameters – including the \"true\" values of the data reflected by noisy measurements – are treated probabilistically (your degree of knowledge about the universe changes as you gather more data).\n",
    "\n",
    "\n",
    "**Example**: The measurement of the flux of a star.\n",
    "\n",
    "- For frequentists, **probability** only has meaning in terms of a **limiting case of repeated measurements**. That is, if I measure the photon flux F from a given star (we'll assume for now that the star's flux does not vary with time), then measure it again, then again, and so on, each time I will get a slightly different answer due to the statistical error of my measuring device. In the limit of a large number of measurements, the frequency of any given value indicates the probability of measuring that value. For frequentists probabilities are fundamentally related to **frequencies of events**. This means, for example, that in a strict frequentist view, it is *meaningless* to talk about the probability of the true flux of the star: the true flux is (by definition) a single fixed value, and to talk about a frequency distribution for a fixed value is nonsense.\n",
    "\n",
    "- For Bayesians, the concept of probability is extended to cover degrees of certainty about statements. Say a Bayesian claims to measure the flux F of a star with some probability P(F): that probability can certainly be estimated from frequencies in the limit of a large number of repeated experiments, but this is not fundamental. The probability is a statement of my knowledge of what the measurement result will be. For Bayesians, probabilities are **fundamentally related to our own knowledge about an event**. This means, for example, that in a Bayesian view, we can meaningfully talk about the probability that the true flux of a star lies in a given range. That probability codifies our knowledge of the value based on prior information and/or available data.\n",
    "\n",
    "In summary: \n",
    "\n",
    "- *Frequentist inference*: estimating an error on a parameter means: \"how much would the parameter change if I had other data\". \n",
    "- *Bayesian inference*: estimating an error on a parameter really means \"deriving a parameter and uncertainties on it\", something a frequentist cannot say as it striclty does not make sense (there is only one true value of a parameter that he tries to estimate and this does not make sense to speak of an uncertainty on a true value). For that purpose, Bayesian inference uses posterior distribution of the parameters (given the data) ($p({\\boldsymbol{\\theta}} , D)$. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. The Bayesian Problem Setting\n",
    "\n",
    "Thus the end-goal of a Bayesian analysis is a probabilistic statement about the universe.\n",
    "Roughly we want to measure\n",
    "\n",
    "$$\n",
    "P(science)\n",
    "$$\n",
    "\n",
    "Where \"science\" might be encapsulated in the cosmological model, the mass of a planet around a star, or whatever else we're interested in learning about.\n",
    "\n",
    "We don't of course measure this without reference to data, so more specifically we want to measure\n",
    "\n",
    "$$\n",
    "P(science~|~data)\n",
    "$$\n",
    "\n",
    "which should be read \"the probability of the science *given* the data.\"\n",
    "\n",
    "Of course, we should be explicit that this measurement is not done in a vaccum: generally before observing any data we have *some* degree of background information that informs the science, so we should actually write\n",
    "\n",
    "$$\n",
    "P(science~|~data, background\\ info)\n",
    "$$\n",
    "\n",
    "This should be read \"the probability of the science given the data *and* the background information\".\n",
    "\n",
    "Finally, there are often things in the scientific model that we don't particularly care about: these are known as \"nuisance parameters\". As an example of a nuisance parameter, if you are finding a planet in radial velocity data, the secular motion of the star is *extremely* important to model correctly, but in the end you don't really care about its best-fit value.\n",
    "\n",
    "With that in mind, we can write:\n",
    "\n",
    "$$\n",
    "P(science,nuisance\\ parameters~|~data, background\\ info)\n",
    "$$\n",
    "\n",
    "Where as before the comma should be read as an \"and\".\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mathematically, we write this down:\n",
    "\n",
    "$$\n",
    "P(\\boldsymbol{\\theta}_S, \\boldsymbol{\\theta}_N~|~D, I)\n",
    "$$\n",
    "\n",
    "- $\\boldsymbol{\\theta}_S$ represents the \"science\": the set of parameters that we are interested in constraining\n",
    "- $\\boldsymbol{\\theta}_N$ represents the \"nuisance parameters\": the set of parameters that are important in the model, but are not particularly interesting for the scientific result.\n",
    "- $D$ represents the \"observed data\"\n",
    "- $I$ represents the information or knowledge you had before observing the data, including whatever made you choose the model you're fitting.\n",
    "\n",
    "Finally, we'll often just write $\\boldsymbol{\\theta} = (\\boldsymbol{\\theta}_S, \\boldsymbol{\\theta}_N)$ as a shorthand for all the model parameters.\n",
    "\n",
    "This quantity, $P(\\boldsymbol{\\theta}~|~D,I)$ is called the \"posterior probability\" and determining this quantity is the ultimate goal of a Bayesian analysis.\n",
    "\n",
    "Now all we need to do is compute it!\n",
    "\n",
    "The core problem is this: **We do not have a way to directly calculate** $P(\\boldsymbol{\\theta}~|~D,I)$. We often do have an expression for $P(D~|~\\boldsymbol{\\theta},I)$, but these two expressions are **not** equal.\n",
    "\n",
    "$$\n",
    "P(\\boldsymbol{\\theta}~|~D,I) \\ne P(D~|~\\boldsymbol{\\theta},I)\n",
    "$$\n",
    "\n",
    "\n",
    "The way these two expressions are related is through the Bayes' theorem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III The Bayes' theorem / Bayes' Rule\n",
    "\n",
    "The definition of conditional probability is entirely symmetric, so we can write\n",
    "\n",
    "$$\n",
    "P(A, B) = P(B, A)\n",
    "$$\n",
    "\n",
    "$$\n",
    "P(A\\mid B)P(B) = P(B\\mid A)P(A)\n",
    "$$\n",
    "\n",
    "which is more commonly rearranged in this form:\n",
    "\n",
    "$$\n",
    "P(A\\mid B) = \\frac{P(B\\mid A)P(A)}{P(B)}\n",
    "$$\n",
    "\n",
    "This is known as *Bayes' Theorem* or *Bayes' Rule*, and is important because it gives a formula for \"flipping\" conditional probabilities.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we replace these labels (* This is were the devel of the controversy is hidden*), we find the usual expression of Bayes' theorem as it relates to model fitting:\n",
    "\n",
    "$$\n",
    "P(\\boldsymbol{\\theta} \\mid D) = \\frac{P(D\\mid \\boldsymbol{\\theta})P(\\boldsymbol{\\theta})}{P(D)}\n",
    "$$\n",
    "\n",
    "Technically all the probabilities should all be conditioned on the information $I$:\n",
    "\n",
    "$$\n",
    "P(\\boldsymbol{\\theta} \\mid D,I) = \\frac{P(D \\mid \\boldsymbol{\\theta},I)P(\\boldsymbol{\\theta} \\mid I)}{P(D \\mid I)}\n",
    "$$\n",
    "\n",
    "Recall $\\boldsymbol{\\theta}$ is the model we're interested in, $D$ is the observed data, and $I$ encodes all the prior information, including what led us to choose the particular model we're using."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*What is controversial in that expression ?* \n",
    "\n",
    "- We have a probability distribution over model parameters. A frequentist would say this is meaningless!\n",
    "\n",
    "- The answer depends on the prior $P(\\theta\\mid I)$. This is the probability of the model parameters without any data: how are we supposed to know that?\n",
    "\n",
    "Nevertheless, applying Bayes' rule in this manner gives us a means of quantifying our knowledge of the parameters $\\theta$ given observed data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III.1 Exploring the Terms in Bayesian Inference\n",
    "\n",
    "We have four terms in the above expression, and we need to make sure we understand them:\n",
    "\n",
    "#### $P(\\boldsymbol{\\theta}\\mid D, I)$ is the *posterior*.\n",
    "This is the quantity we want to compute: our knowledge of the model given the data & background knowledge (including the choice of model).\n",
    "\n",
    "#### $P(D\\mid\\boldsymbol{\\theta},I)$ is the *likelihood*.\n",
    "This measures the probability of seeing our data given the model. This is identical to the quantity maximized in frequentist *maximum-likelihood* approaches.\n",
    "\n",
    "#### $P(\\boldsymbol{\\theta}\\mid I)$ is the *prior*.\n",
    "This encodes any knowledge we had about the answer before measuring the current data.\n",
    "\n",
    "#### $P(D\\mid I)$ is the *Fully Marginalized Likelihood* (or *Evidence*)\n",
    "You might prefer the acronym *FML* (it's also called the *Evidence* - namely the evidence that the data D was generated by the model - among other things). Its complete expression is:\n",
    "\n",
    "$$\n",
    "P(D\\mid I) = \\int P(D\\mid\\boldsymbol{\\theta}, I) P(\\boldsymbol{\\theta}) d\\boldsymbol{\\theta}\n",
    "$$\n",
    "\n",
    "In the context of **model fitting**, it acts as a normalization constant and in most cases can be ignored. In **model selection**, the FML can become important (but it is costly to calculate as you need to evaluate the likelihood for all the values of your parameters $\\boldsymbol{\\theta}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III.2 What is the Point?\n",
    "\n",
    "At first blush, this might all seem needlessly complicated. Why not simply maximize the likelihood and be done with it? Why multiply by a prior at all?\n",
    "\n",
    "- *Purity*: you quantify knowledge in terms of a probability, then follow the math to compute the answer. The fact that you need to specify a prior might be inconvenient, but we can't simply pretend it away.\n",
    "- *Parameter Uncertainties*: Whether frequentist or Bayesian, the maximum likelihood \"point estimate\" is only a small part of the picture. What we're really interested in scientifically is the uncertainty of the estimates. So simply reporting a point estimate is not appropriate. In frequentist approaches, \"error bars\" are generally computed from *Confidence Intervals*, which effectively measure the probability that the data encompass the true (fixed) value of the parameter, hence $P(\\hat{\\boldsymbol{\\theta}}\\mid\\boldsymbol{\\theta})$, rather than $P(\\boldsymbol{\\theta}\\mid D)$, which is effectively what we are interested in (and what is derived from the Bbayesian approach). Note the difference: the Bayesian solution is a statement of probability about the parameter value given fixed bounds. The frequentist solution is a probability about the bounds given a fixed parameter value. This follows directly from the philosophical definitions of probability that the two approaches are based on.\n",
    "- *Marginalization and Nuisance Parameters*: Bayesian approaches offer a very natural way to systematically account for nuisance parameters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IV. Simple Bayesian Modeling\n",
    "\n",
    "We'll start with the classic model fitting problem: **Fitting a line to data**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the sake of illustration, we will generate a fake data set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function that generates points following a linear tred. \n",
    "def make_data(intercept, slope, N=20, dy=5, rseed=42):\n",
    "    '''\n",
    "    Parameters:\n",
    "    -----------\n",
    "    intercept, slope: parameters of the linear model\n",
    "    dy: width of normally distributed data points around the \"true line\"\n",
    "    rseed: Seed of the random number generator (fixed for the sake of discussion)\n",
    "    Output:\n",
    "    -------\n",
    "    x, y, sig_y \n",
    "    '''\n",
    "    rand = np.random.RandomState(rseed)\n",
    "    x = 100 * rand.rand(N)\n",
    "    y = intercept + slope * x\n",
    "    y += dy * rand.randn(N)\n",
    "    return x, y, dy * np.ones_like(x)\n",
    "\n",
    "theta_true = [25, 0.5]\n",
    "x, y, dy = make_data(theta_true[0], theta_true[1])  # could also be make_data(*theta_true)\n",
    "plt.errorbar(x, y, dy, fmt='o')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IV.1 The model\n",
    "\n",
    "Next we need to specify a model. We're fitting a straight line to data, so we'll need a slope and an intercept; i.e.\n",
    "\n",
    "$$\n",
    "y_M(x) = \\theta_0 + \\theta_1\\,x \n",
    "$$\n",
    "\n",
    "where our paramteter vector might be \n",
    "\n",
    "$$\n",
    "\\boldsymbol{\\theta} = [\\theta_0, \\theta_1]\n",
    "$$\n",
    "\n",
    "But this is only half the picture: what we mean by a \"model\" in a Bayesian sense is not only the expected value $<y(x;\\boldsymbol{\\theta})>$, but a **probability distribution** for our data.\n",
    "That is, we need an expression to compute the likelihood $P(D\\mid\\theta)$ for our data as a function of the parameters $\\boldsymbol{\\theta}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may remember from the [previous lecture](../Lecture_6/Lecture_6.ipynb) that the likelihood for n (independent) points ($y_i$) drawn from a normal distribution:\n",
    "$$\n",
    "y_i \\sim N(y_M(x_i; \\boldsymbol{\\theta}), \\sigma)\n",
    "$$\n",
    "the likelihood is the product of the probabilities for each single data point, namely (generalised for heteroscedastic errors):\n",
    "\n",
    "$$\n",
    "p(y_i\\,\\mid\\,\\boldsymbol{\\theta} ) = \\frac{1}{\\sigma \\sqrt{2\\,\\pi}} \\, \\exp\\left[-0.5 \\left (\\frac{y_i - y_M(x_i; \\boldsymbol{\\theta})}{\\sigma} \\right)^2\\right] \n",
    "$$\n",
    "\n",
    "Hence:\n",
    "\n",
    "$$\n",
    "L \\equiv P(D\\mid\\boldsymbol{\\theta}) = \\prod_{i=1}^N P(x_i,y_i\\mid\\boldsymbol{\\theta})\n",
    "$$\n",
    "\n",
    "$$\n",
    "P(D\\mid\\boldsymbol{\\theta}) =  \\prod_{i=1}^{N} \\frac{1}{\\sqrt{2\\,\\pi \\sigma_i^2}} \\, \\exp\\left[\\left (\\frac{ -(y_i - (\\theta_0+\\theta_1\\,x_i))^2}{2\\,\\sigma_i^2} \\right)\\right]\n",
    "$$\n",
    "\n",
    "Or, written in terms of log-likelihood:\n",
    "\n",
    "$$\n",
    "\\ln(P(D\\mid\\boldsymbol{\\theta})) \\, = -\\frac{1}{2}\\,\\sum_{i=1}^{N} \\left (\\ln(2\\pi\\sigma_i^2) + \\frac{ (y_i - (\\theta_0+\\theta_1\\,x_i))^2}{\\sigma_i^2} \\right) \n",
    "$$\n",
    "\n",
    "We can now write a function that computes the log-likelihood given a parameter vector $\\boldsymbol{\\theta}$, an array of errors $\\sigma_i$, and an array of $x$ and $y$ values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ln_likelihood(theta, x, y, dy):\n",
    "    y_model = theta[0] + theta[1] * x\n",
    "    return -0.5 * np.sum(np.log(2 * np.pi * dy ** 2) + (y - y_model) ** 2 / dy ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The frequentist approach would be to search for the maximum of the likelihood. The Bayesian approach is more comprehensive. It is not a matter of finding the most likely estimator of $\\theta$ but of deriving the **full posterior probability** $P(\\boldsymbol{\\theta}~\\mid~D)$. For that purpose we need to set a prior ... "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IV. 2 The prior\n",
    "\n",
    "### IV.2.1 Conjugate priors\n",
    "\n",
    "In the early days of Bayesian analysis, people were considering [conjugate prior](https://en.wikipedia.org/wiki/Conjugate_prior). A *conjugate prior* is a prior which, due to its mathematical relation to the likelihood, makes the result analytically computable. Those priors are rarely defensible on any grounds other than computational convenience, and so this is **almost never a good choice**.\n",
    "\n",
    "### IV.2.2 Empirical priors\n",
    "\n",
    "Empirical Priors are priors which are actually posteriors from previous studies of the same phenomenon. For example, it's common in Supernova cosmology studies to use the WMAP results as a prior: that is, we actually plug-in a *real result* and use our new data to improve on that. This situation is where Bayesian approaches really shine.\n",
    "\n",
    "For our linear fit, you might imagine that our $x, y$ data is a more accurate version of a previous experiment, where we've found that the intercept is $\\theta_0 = 50 \\pm 30$ and the slope is $\\theta_1 = 1.0 \\pm 0.5$.\n",
    "In this case, we would encode this prior knowledge in the prior distribution itself.\n",
    "\n",
    "### IV.2.3 Flat Priors\n",
    "\n",
    "If you don't have an empirical prior, you might be tempted to simply use a *flat prior* – i.e. a prior that is constant between two reasonable limits (i.e. equal probability slopes from -1000 to +1000).\n",
    "\n",
    "The problem is that flat priors are not always non-informative! For example, a flat prior on the slope will effectively give a higher weight to larger slopes.\n",
    "We can see this straightforwardly by plotting regularly-spaced slopes between 0 and 20:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xx = np.linspace(-1, 1)\n",
    "for slope in np.linspace(0, 20, 100):\n",
    "    plt.plot(xx, slope * xx, '-k', linewidth=1)\n",
    "plt.axis([-1, 1, -1, 1], aspect='equal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The density of the lines is a proxy for the probability of those slopes with a flat prior.\n",
    "This is an important point to realize: **flat priors are not necessarily minimally informative**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IV.2.4 Non-informative Priors\n",
    "\n",
    "What we *really* want in cases where no empirical prior is available is a **non-informative prior**. Among other things, such a prior should not depend on the units of the data.\n",
    "Perhaps the most principled approach to choosing non-informative priors was the *principle of maximum entropy* advocated by Jaynes ([book](http://omega.albany.edu:8008/JaynesBook.html)).\n",
    "\n",
    "Similar in spirit is the commonly-used [Jeffreys Prior](https://en.wikipedia.org/wiki/Jeffreys_prior), which in many cases of interest amounts to a \"scale invariant\" prior: a flat prior on the logarithm of the parameter.\n",
    "\n",
    "In the case of the linear slope, we often want a prior which does not artificially over-weight large slopes: there are a couple possible approaches to this (see http://arxiv.org/abs/1411.5018 for some discussion). For our situation, we might use a flat prior on the angle the line makes with the x-axis, which gives\n",
    "\n",
    "$$\n",
    "P(\\theta_1) \\propto (1 + \\theta_1^2)^{-3/2}\n",
    "$$\n",
    "\n",
    "For lack of a better term, we can call this a \"symmetric prior\" on the slope. Indeed, it is the same whether we are fitting $y = \\theta_1 \\, x + \\theta_0$ or $x = \\theta_1^\\prime\\,y + \\theta_0^\\prime$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IV.3 Implementation\n",
    "\n",
    "Let's define two python functions to compute the options for our prior: we'll use both a (log) flat prior and a (log) symmetric prior.\n",
    "In general, we need not worry about the normalization of the prior or the likelihood, which makes our lives easier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ln_flat_prior(theta):\n",
    "    '''\n",
    "    theta = shape(2) array containing the [intercept, slope] = [theta_0, theta_1]\n",
    "    '''\n",
    "    if np.all(np.abs(theta[1]) < 1000):\n",
    "        return 0 # log(1)\n",
    "    else:\n",
    "        return -np.inf  # log(0)\n",
    "    \n",
    "def ln_symmetric_prior(theta):\n",
    "    '''\n",
    "    theta = shape(2) array containing the [intercept, slope] = [theta_0, theta_1]\n",
    "    '''\n",
    "    if np.abs(theta[0]) < 1000:\n",
    "        return -1.5 * np.log(1 + theta[1] ** 2)\n",
    "    else:\n",
    "        return -np.inf  # log(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With these defined, we now have what we need to compute the log posterior as a function of the model parameters.\n",
    "You might be tempted to maximize this posterior in the same way that we did with the likelihood above. However, the proper Bayesian approach is not just a matter of getting the maximum of the posterior of our parameters but aims at getting the (possibly marginalized) posterior probability for our parameters. \n",
    "\n",
    "Remember that the posterior probability on the parameters is the product of the likelihood and of the prior. Hence the log of the posterior is the sum of the of the log of these two quantities. For the simple case above, we can explicitly evaluate the log probability on a grid for the 2 parameters of our model. \n",
    "\n",
    "#### Exercise:\n",
    "\n",
    "1. Using matplotlib, plot the posterior probability distribution for the slope and intercept, once for each prior. I would suggest using ``plt.contourf()`` or ``plt.pcolor()``. How different are the distributions?\n",
    "\n",
    "2. Modify the dataset – how do the results change if you have very few data points or very large errors?\n",
    "\n",
    "3. If you finish this quickly, try adding 1-sigma and 2-sigma contours to your plot, keeping in mind that the probabilities are not normalized! You can add them to your plot with ``plt.contour()``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use the cells below to evaluate the log likelihood for each of the two priors above\n",
    "def ln_P1(x, y, dy, slope_limits=[0.3, 0.7], intercept_limits=[15,35]):\n",
    "    '''\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the present case, you'll see that the form of the prior makes little difference on the results. This is often, but not always the case if you have enough data to constrain your model. Instead, if you have few data and/or large error bars, your results is more likely to be prior dependent which means that, if you do not have good data, you won't improve much your knowledge about the world you are probing with your data !  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V. Bayesian Modeling with MCMC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous section we explored a Bayesian solution to a straight line fit.\n",
    "The result made use of the evaluation of a posterior across a grid of parameters: a strategy that *will not* scale to higher-dimensional models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V.1 The Curse of dimensionality\n",
    "\n",
    "The reason it will not scale is one of the effects of the ubiquitous \"Curse of Dimensionality\". To understand this, consider how many evaluations we need for an $N$-dimensional grid with 100 samples per dimension\n",
    "\n",
    "In one dimension, we have $100$ points.\n",
    "\n",
    "In two dimensions we have $100^2 = 10,000$ evaluations.\n",
    "\n",
    "In three dimensions, we have $100^3 = 1,000,000$ evaluations.\n",
    "\n",
    "In $N$ dimensions, we have $100^N$ evaluations, and as $N$ grows this quickly becomes untenable! For example, if we have only six model parameters, this \"dense grid\" approach will require evaluating the posterior at one trillion grid points, the results of which would require several terabytes of memory just to store!\n",
    "\n",
    "Evidently the dense grid strategy will not work for any but the simplest Bayesian models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V.2 Circumventing the Curse with Sampling\n",
    "\n",
    "An idea that revolutionized Bayesian modeling (and made possible the wide variety of Bayesian approaches used in practice today) is *Markov Chain Monte Carlo* (MCMC), an approach that allows one to efficiently draw (pseudo)random samples from a posterior distribution even in relatively high dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### V.2.1 The Metropolis-Hastings Sampler\n",
    "\n",
    "Perhaps the simplest of MCMC samplers is the *Metropolis-Hastings Sampler*.\n",
    "This provides a procedure which, given a pseudo-random number generator, selects a chain of points which (in the long-term limit) will be a representative sample from the posterior. The procedure is surprisingly simple:\n",
    "\n",
    "1. Define a posterior $p(\\theta~|~D, I)$\n",
    "2. Define a *proposal density* $p(\\theta_{i + 1}~|~\\theta_i)$, which must be a symmetric function, but otherwise is unconstrained (a Gaussian is the usual choice).\n",
    "3. Choose a starting point $\\theta_0$\n",
    "4. Repeat the following:\n",
    "\n",
    "   1. Given $\\theta_i$, draw a new $\\theta_{i + 1}$\n",
    "   \n",
    "   2. Compute the *acceptance ratio*\n",
    "      $$\n",
    "      a = \\frac{p(\\theta_{i + 1}~|~D,I)}{p(\\theta_i~|~D,I)}\n",
    "      $$\n",
    "   \n",
    "   3. If $a \\ge 1$, the proposal is more likely: accept the draw and add $\\theta_{i + 1}$ to the chain.\n",
    "   \n",
    "   4. If $a < 1$, then accept the point with probability $a$: this can be done by drawing a uniform random number $r$ and checking if $a < r$. If the point is accepted, add $\\theta_{i + 1}$ to the chain. If not, then add $\\theta_i$ to the chain *again*.\n",
    "\n",
    "There are a few caveats to be aware of when using MCMC\n",
    "\n",
    "#### 1. The procedure is provably correct... but only in the long-term limit!\n",
    "\n",
    "Sometimes the long-term limit is **very** long. What we're looking for is \"stabilization\" of the MCMC chain, meaning that it has reached a statistical equilibrium. There is a vast literature on how to measure stabilization of an MCMC chain. Here we'll use the sloppy but intuitive LAI approach (i.e. Look At It).\n",
    "\n",
    "#### 2. The size of the proposal distribution is *very* important\n",
    "\n",
    "- If your proposal distribution is too small, it will take too long for your chain to move, and you have the danger of getting stuck in a local maximum for a long (but not infinite) time.\n",
    "\n",
    "- If your proposal distribution is too large, you will not be able to efficiently explore the space around a particular peak\n",
    "\n",
    "In general, choosing an appropriate scale for the proposal distribution is one of the most difficult parts of using the MCMC procedure above.\n",
    "More sophisticated methods (see later) have built-in ways to estimate this along the way, but it's still something to be aware of!\n",
    "\n",
    "#### 3. Fast Stabilization can be helped by good initialization\n",
    "\n",
    "In practice, assuring that MCMC will stabilize quickly has a lot to do with choosing a suitable initialization. For this purpose, it can be useful to find the maximum a posteriori (MAP) value, and initialize the chain with this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### V.2.2 Sampling with ``emcee``\n",
    "\n",
    "There are several good Python approaches to Bayesian computation with MCMC. \n",
    "Here we'll focus on [``emcee``](http://dan.iel.fm/emcee/), a lightweight Python package developed by Dan Foreman-Mackey and collaborators.\n",
    "One benefit of ``emcee`` is that it uses an *ensemble sampler* which automatically tunes the shape and size of the proposal distribution (you can read more details in the ``emcee`` documentation).\n",
    "\n",
    "Let's apply MCMC to our simple line fitting problem. The following steps are required: \n",
    "\n",
    "#### V2.2.1 Expression of the posterior (likelihood and prior)\n",
    "\n",
    "For that purpose, we need first to define a function that enables us to evaluate the posterior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ln_posterior(theta, x, y, dy):\n",
    "    return ln_flat_prior(theta) + ln_likelihood(theta, x, y, dy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### V.2.2.2 Using emcee to sample the posterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import emcee\n",
    "\n",
    "ndim = 2  # number of parameters in the model\n",
    "nwalkers = 50  # number of MCMC walkers\n",
    "\n",
    "# initialize walkers\n",
    "starting_guesses = np.random.randn(nwalkers, ndim)\n",
    "\n",
    "sampler = emcee.EnsembleSampler(nwalkers, ndim, ln_posterior, args=[x, y, dy])\n",
    "\n",
    "pos, prob, state = sampler.run_mcmc(starting_guesses, 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the [corner.py](https://pypi.python.org/pypi/corner) package, we can take a look at this multi-dimensional posterior, along with the input values for the parameters:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAFuCAYAAAChovKPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nEGcYw20wGAxxhjHcBoPBEGf8f+tJjGnqDa3mAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x99bd9b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import corner\n",
    "corner.corner(sampler.flatchain, labels=['intercept', 'slope'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### V.2.2.3 Plotting the chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, sharex=True)\n",
    "for i in range(2):\n",
    "    ax[i].plot(sampler.chain[:, :, i].T, '-k', alpha=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### V.2.2.4 Restarting after burn-in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sampler.reset()\n",
    "pos, prob, state = sampler.run_mcmc(pos, 1000)\n",
    "\n",
    "fig, ax = plt.subplots(2, sharex=True)\n",
    "for i in range(2):\n",
    "    ax[i].plot(sampler.chain[:, :, i].T, '-k', alpha=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### V.2.2.5 Visualizing the posterior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A way to visualize the posterior is to plot the model over the data.\n",
    "Each point in the two-dimensional space ($\\theta_0$,$\\theta_1$) explored by the sampler, corresponds to a possible model for our data. If we select ~100 of these at random and plot them over our data, it will give us a good idea of the spread in the model results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "chain = sampler.flatchain\n",
    "\n",
    "plt.errorbar(x, y, dy, fmt='o');\n",
    "\n",
    "thetas = [chain[i] for i in np.random.choice(chain.shape[0], 100)]\n",
    "\n",
    "xfit = np.linspace(0, 100)\n",
    "for i in range(100):\n",
    "    theta = thetas[i]\n",
    "    plt.plot(xfit, theta[0] + theta[1] * xfit, color='black', alpha=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### V.2.3. Fitting a straight line with intrinsic scatter\n",
    "\n",
    "Above we have done a simple model, where the data is drawn from a straight line.\n",
    "\n",
    "Often, however, we will be modeling relationships where there is some intrinsic scatter in the model itself: that is, even if the data were *perfectly* measured, they would not fall along a perfect straight line, but would have some (unknown) scatter about that line.\n",
    "\n",
    "Here we'll make a slightly more complicated model in which we will fit for the slope, intercept, and intrinsic scatter (i.e. intrinsic scatter is a parameter of our model) all at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_data_scatter(intercept, slope, scatter, N=20, dy=2, rseed=42):\n",
    "    rand = np.random.RandomState(rseed)\n",
    "    x = 100 * rand.rand(20)\n",
    "    y = intercept + slope * x\n",
    "    y += np.sqrt(dy ** 2 + scatter ** 2) * rand.randn(20)\n",
    "    return x, y, dy * np.ones_like(x)\n",
    "\n",
    "theta = (25, 0.5, 3.0)  # (intercept, slope, intrinsic scatter)\n",
    "x, y, dy = make_data_scatter(*theta)\n",
    "plt.errorbar(x, y, dy, fmt='o')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now go through the different steps to solve this problem in a Bayesian way, and use MCMC to evaluate the posterior on the parameters. \n",
    "\n",
    "#### V.2.3.1 Defining the likelihood and prior\n",
    "\n",
    "You are now getting familiar with the first step of the procedure which consists in deriving an expression for the likelihood. The likelihood for this model looks very similar to what we used above, except that the intrinsic scatter is added *in quadrature* to the measurement error.\n",
    "\n",
    "\n",
    "$$\n",
    "P(D\\mid\\boldsymbol{\\theta}) =  \\prod_{i=1}^{N} \\frac{1}{\\sqrt{2\\,\\pi (\\sigma_i^2 + \\sigma^2)}} \\, \\exp\\left[\\left (\\frac{ -(y_i - (\\theta_0+\\theta_1\\,x_i))^2}{2\\,(\\sigma_i^2+\\sigma^2)} \\right)\\right]\n",
    "$$\n",
    "\n",
    "For the prior, you can use either a flat or symmetric prior on the slope and intercept, but on the intrinsic scatter $\\sigma$ it is best to use a scale-invariant Jeffreys Prior:\n",
    "\n",
    "$$\n",
    "P(\\sigma)\\propto\\sigma^{-1}\n",
    "$$\n",
    "\n",
    "As discussed before, this has the nice feature that the resulting posterior will not depend on the units of measurement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define functions to compute the log-prior, log-likelihood, and log-posterior\n",
    "\n",
    "# theta = [intercept, slope, scatter]\n",
    "\n",
    "def ln_prior(theta):\n",
    "    # fill this in\n",
    "    return\n",
    "    \n",
    "def ln_likelihood(theta, x, y, dy):\n",
    "    # fill this in\n",
    "    return\n",
    "\n",
    "def ln_posterior(theta, x, y, dy):\n",
    "    # fill this in\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sampling from the Posterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Using emcee, create and initialize a sampler and draw 200 samples from the posterior.\n",
    "# Remember to think about what starting guesses should you use!\n",
    "# You can use the above as a template\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualizing the Chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plot the three chains as above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Resetting and getting a clean sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Are your chains stabilized? Reset them and get a clean sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualizing the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use e.g. corner.py to visualize the three-dimensional posterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Next plot ~100 of the samples as models over the data to get an idea of the fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XX References:\n",
    "\n",
    "**Chapter 5 ** (5.1, 5.2, 5.3, 5.8) of the book <a class=\"anchor\" id=\"book\"></a> *Statistics, data mining and Machine learning in astronomy* by Z. Ivezic et al. in Princeton Series in Modern Astronomy. \n",
    "\n",
    "- This notebook includes a large fraction of the material that J. Vander Plas gave during the \"Bayesian Methods in Astronomy workshop\", presented at the 227th meeting of the American Astronomical Society. The full repository with that material can be found on GitHub: http://github.com/jakevdp/AAS227Workshop\n",
    "\n",
    "- More insights on the differences between frequentist and Bayesian approaches: see [J. VanderPlass blog posts](http://jakevdp.github.io/blog/2014/03/11/frequentism-and-bayesianism-a-practical-intro/) \n",
    "\n",
    "- Jayes: [*Probability Theory: The Logic of Science*](http://bayes.wustl.edu/etj/prob/book.pdf).\n",
    "\n",
    "- For some approachable reading on frequentist vs. Bayesian uncertainties, I'd suggest [The Fallacy of Placing Confidence in Confidence Intervals](https://learnbayes.org/papers/confidenceIntervalsFallacy/), as well as Jake VanderPlast blog post on the topic, [Confidence, Credibility, and why Frequentism and Science do not Mix](http://jakevdp.github.io/blog/2014/06/12/frequentism-and-bayesianism-3-confidence-credibility/).\n",
    "\n",
    "- Foreman-Mackey et al. 2012 [*EMCEE, the MCMC hammer*](https://arxiv.org/abs/1202.3665) ; see also http://dan.iel.fm/emcee/current/\n",
    "\n",
    "- About the variety of approaches to MCMC: Allison and Dunkley 2013: [Comparison of sampling techniques for Bayesian parameter estimation](https://arxiv.org/abs/1308.2675). See also [How to Be a Bayesian in Python](http://jakevdp.github.io/blog/2014/06/14/frequentism-and-bayesianism-4-bayesian-in-python/). \n",
    "\n",
    "- Andreon 2011 [Understanding better (some) astronomical data using Bayesian methods](https://arxiv.org/abs/1112.3652)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
